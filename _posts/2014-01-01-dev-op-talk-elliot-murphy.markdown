---
layout: post
title:  "DevOps and HIPAA Complaince at a startup - a talk with Elliot Murphy"
date:   2014-01-01 16:07:00
categories: devops hipaa chef vagrant
---

My talk today is with Elliot Murphy, the lead (and only full time developer) with a small startup in Behavioral Healthcare. We cover a lot of ground in our talk, including the challenges of building HIPAA compliant applications, managing risk, encryption and security, developer work flow, server configuration, deployment, databases, and testing.

<!--more-->

This is the first in a series of interviews around the topic of DevOps.  My goal is to document the approach different organizations take as they build and maintaining server infrastructure through automation. I want to try to understand how various organizations approach the complexity of maintaining server environments, credentials, backups, and the process for moving code from a developers hands into production.

* <iframe width="560" height="315" src="//www.youtube.com/embed/rzlhqS6CRDY" frameborder="0" allowfullscreen></iframe>

**Technologies Used:** Git, Ruby, Rails, MySQL, MongoDB, Nginx, Solr, Elastic Search, Delayed Job, Capistrano, Vagrant, Chef Solo, Ubuntu, RSpec, Cucumber

**Services Used:** Heroku, GitHub, Amazon AWS, Digital Ocean, Paper Trail, Code Climate, Travis

**Jason**: Welcome, this will be the first talk we'll do around DevOps.  The goal of these talks is to talk with developers, primarily from small companies, to try and figure out what's working for them, what things are not working for them, what they like, what they don't like, and how they are solving the same set of problems.  My name is Jason Vanderhoof.  Today I'm talking with Elliot Murphy.  Elliot, if you could give us a quick overview of your background and where you are currently?

**Elliot**: I've been doing software all my life, professionally for about 20 years. The last nine years, I've been doing a lot of web facing stuff.  About nine years ago I started working at MySQL. From there I moved over to Canonical, working with some of their web services teams, doing code hosting and developer tools and things like that.  I also got to build out a file sharing service called Ubuntu One. In the last two years, I switched over to health care (I've had this goal of working at smaller and smaller companies for much of my career). Now I'm at this little, tiny company of nine people, totally bootstrapped.  The founder is Pat Deegan, and she is a psychologist. There are a couple of Rails apps we have that are totally focused on helping people with severe and persistent mental disorders get better care from their doctors. Technically speaking, the apps are pretty straight forward. In terms of scale, there's not a lot to them.  Where we get a big extra complication, is that we're dealing with medical data, so we have to be compliant with all the HIPAA rules and regulations and due diligence.

**Jason**: Could you dive in a little more into some of the applications, what are traffic levels that you're seeing, etc?

**Elliot**: It's interesting, with this system where I end up dealing with scale is the scaling down side of things.  We have a single database server.  We have a single web front end.  We have a single proxy, and it handles load just fine. In terms of number of users, we have around twenty-five thousand patients in our system, and that's spread across 60 or so clinics across the US. We're not hitting traffic load issues at all.  One of the things I actually struggle with a little bit is seeing some tools that are really great for large installations, and figuring out how can I take some of that goodness and use it on a one, two, three developer team.

We have two products.  One product we specifically designed not to include any medical or personal health information, or any personally identifying information. That was a interesting as a design constraint. We went through the product and moved any instance of personal data into the other product. I get to use some more modern tools on the non HIPAA product. It's called Recovery Library. I run that one on Heroku.  I use Papertrail for logging, and a bunch of really modern stuff.  The HIPAA compliant one, I have a lot more limitations. I have to buy much more expensive hosting from a company that's willing to sign a Business Associates agreement with us.  We have much more stringent audit requirements with that product. I can't use any third party services, where some of the medical information potentially might end up, without signing a Business Associates agreement with those vendors. The contrast makes it really interesting, I get to dabble and play the new stuff, and then see how I might apply some of the other application. For us, it's not really a cost savings to running open source versions of some of the SAAS stuff that's out there on the market, but it's purely about a data control.  Papertrail, for example, just doesn't offer BA agreements, so I'm looking at centralized logging solution like Log Stash and Kibana, not because I'm paying so much for the service, but just because I really do need to be able to put my hand up and swear I know where that data is and who's accessed it.

**Jason**: Yeah, that is quite a challenge. Run me through some of the moving parts, what are you using?

**Elliot**: I think we have a pretty standard Rails app. If you squint at it a certain way, it is contact management system, and if you look at it another way, it's this mobile kiosk that's used in the field. It's a Rails front-end. For one of the products we're using MongoDB, and on one of the product we're using MySQL. On the older product we're using Solr for search, On the newer product we're using Elastic Search. We're also using Delayed Job, sort of standard stuff. We're using in database encryption on MySQL. It's terrible. We're planning to get rid of that. I'm totally not happy with MongoDB or MySQL. I think both of them are ok products.  Where I want to go is to keep using Elastic Search. I want to move both my MongoDB stuff and my MySQL stuff into Postgres. I've been following along very closely with both the JSON and the Hstore development work that's been happening there on Postgres. 9.3 is really close. I think once Hstore and JSON merge together, it'll be fantastic. There is this new presentation that came out a few weeks ago called PGRest. It's about putting a MongoDB compatible interface inside Postgres. That's very much what I want to do. We had a genuine need for some of that document oriented stuff that's in MongoDB, but I like having some of the relational stuff available for Postgres too. I totally trust Postgres. My goal is to get encryption totally out of the database, and move to file system level. I've been looking at eCryptfs. One of the lead developers on eCryptfs, Dustin Kirkland, actually worked at a startup for a little while called Gazzang. They had a nice commercialized version of eCryptfs with some really nice non-technical key management options. I'm not sure if I'll use the Gazzang product or the open source eCryptfs. I really want to put the encryption at a level below the database, so I can run whichever database I want on top of it, and not have to fiddle with in database encryption.

**Jason**: Now for both MySQL and Mongo, what's sort of the driving reason behind a switch. At a former company, we used Postgres heavily, so I see why you'd want to move to Postgres, but specifically are the set of problems you're trying to address?

**Elliot**: As a former MySQL developer, even when I was working at MySQL, a lot of us were impressed at the clarity of the Postgres code base. It has proven to be very maintainable. I don't think MongoDB or MySQL are bad products. I have friends that work on both, and several of the forks of MySQL. When I was at Canonical, was the first time I really ran Postgres. One of the head of operations guys really liked Postgres. We ran some very high loads, and over the course of a few years, it gave me a lot of confidence that Postgres was solid. It didn't surprise us ever. CouchDB surprised us, MongoDB surprised us, MySQL surprised us, but Postgres was just really solid. In the last couple of years, I had high hopes for the Drizzle fork of MySQL, and I think it's a great place to do database experiments. They've taken that fork and turned it into a database construction kit. If I was a database researcher, and looking to experiment with stuff, I would probably go play there. I don't see any features coming out in any of the forks of MySQL that are a perfect custom fit for problems that I have right now. The problem I have right now is I want some of the MongoDB stuff and I don't want to run MongoDB. The fact that all of this JSON support is going into Postgres and that Heroku and AWS has such great hosted Postgres, it just seems like a really practical choice. Databases are like editors. You could argue for hours that this one is better than that one. What it really comes down to is if you have someone on your team who feels that this one is better than that one, then, you should use it. For me right now, for this set of products, Postgres feels like the future for our little company.

So I didn't actually answer your question about why do I want to get rid of Mongo. There are a couple of factors.  One, is that I've had some data corruption issues.  I think those can be worked around by tuning, but it's also a new database to learn and tune. I'm convinced at this point, that I don't want to have to reinvent backups and tuning for another whole database. I'm not getting enough win from Mongo that it's worth adding that whole extra product to have to fit in my head, so just consolidate on one database. I'm not really comfortable giving up Postgres and consolidating on Mongo, because I really want some of the stronger transactional guarantees that I am able to get in Postgres.

**Jason**: That does make a lot of sense.  I did see an interesting post about Mongo where they were looking to do a distributed datastore to do replication across data centers.  Mongo was what they finalized on.  One of the issues was lag time between data centers, primarily with MySQL, they were running into issues where the slaves would disconnect and there was no way to flip over quickly enough.  I've always been a little weary about some of the document stores because I think they solve a very particular problem.  They are new and exciting, or they have historically been new and exciting, and a lot of times they are used to solve problems that at the end of the day if you look at the problem as a whole, a lot of that will fit well into a relational database. With Postgres, honestly, with the JSON store, Hstore, we used that heavily to deal with different looking datasets.

**Elliot**: I think if I were growing up, dealing with wild increases in traffic levels and starting to get really heavy loads on the database, I probably would turn right back around and reintroduce a more specialized nosql store and not put everything in Postgres. I want to be really fair to the engineering team that works on Mongo.  All database are fundamentally broken. It's a question of how each one is broken and how closely you want to pay attention to each one.  There is this fantastic set of in depth, independent analysis of all of the databases of all the databases and nosql stores (http://aphyr.com/).  This guy named Kyle actually shows how Cassandra is broken, Redis is broken, Riak is broken, MySQL is broken, Postgres is broken, in all these interesting and really subtle ways. You just need to know how my database is broken, and how I am going to dealing with that at the business level.

**Jason**: At the end of the day, it's the right tool for the job.  I like how you said earlier that database are like editors, you can get into flame wars and debate forever. Can you talk a little more about how you guys do development, and move code through some sort of staging environment, and then into production.  Especially with HIPAA compliance, how do you deal with keeping a couple of different environments configured and keeping data testable down low, but obviously, still staying compliant in terms of data control?

**Elliot**: Step one with HIPAA compliance is to recognize that there are going to be some grey areas. You do what's called risk assessment with your company's owners. It's not just a technical solution to a technical problem. It's selecting technical solutions for risks as you've evaluated them for your company and for your customers. Our customers are people who are receiving psychiatric care. You can imagine a number of reasons why their privacy being violated would be really damaging, or could be really damaging. You could imagine a partner, a boyfriend, a girlfriend, or parent even, finding out some confidential information about your diagnosis, and that having a devastating effect on the relationship. You can imaging a politician receiving care for something, and it coming out it a bad way. It's a shame really, that psychiatric care, mental illness is one of the last taboos in the US. We react with horror and look sideways at people who have a diagnosis, yet one in four people in the US are taking psychiatric meds. They have some sort of diagnosis of mental illness.  We shouldn't be ashamed of it, but we are. When we're entering into our risk assessment, we take that very seriously. We are representing a vulnerable population with very real side affects, if their data is compromised. It's not some abstract philosophical right to privacy, this is real. We have to balance the obligation to protect and the right to privacy, with the right to disclose. I find that this is where a lot of other software companies that deal with privacy get it wrong. There's this concept called the dignity of risk. As someone receiving care, if I choose to disclose, it should be very easy for me to actually get my data, get the notes and my treatment history, and share it with other people, whether those are doctors, loved ones, or whoever. It's important that we not just build giant fences around the data but also remember that if it needs to, it should be very easy for people to get their data. The third thing is, we need to let people see who has seen their data. That, I think, ends up being the key. That is the central focus, around which all of our technical controls get hung. If my doctor or my case worker have looked at my files, that's great. If Joe Random, who happens to work at the same hospital, is looking at my files, why is that? What's happened? We look at what could happen with the data, and then where are the different places that people could break in to access that data. Someone could break through our firewall, get into the database, and then crack the database encryption. Then what happens? How many people are effected? What could they do with that data? Contrast that with another threat model: Someone at a clinic writes their password on a sticky note and leaves it on a monitor, or someone at a clinic logs in a client and forgets to switch to the new client before bring the next client in. You can trace all of these scenarios. When you think about the whole spectrum of threats, all of a sudden you realize that a friendlier UI around switching users would do more to protect data then putting in a Fort Knox double layer encryption on the database. You have to take this really practical assessment of risk across every single interaction with the data, and everywhere this data flows. You try and pretend you're a piece of this data and think where might you go. Once we figure out all of those risk areas, we try and draw a very bright line around the HIPAA protected data. So we try and do as much of business logic as possible outside of that data set. We try and keep it really contained. Only this product has HIPAA protected data and we only have it living in these places, and we know the people and companies who have access to that data, including sub-contractors. Our customers are the hospitals and clinics. They are the ones who are actually under obligation by law. They are the covered entity in HIPAA terms. They have to protect their patients' data. They give that data to us as a vendor to protect. We sign what's called a Business Associates Agreement. So that's a trickle down thing. The initial burden is on the covered entity. We are a vendor to them, so we have a Business Associates Agreement, and then anybody that we do business with over the course of fulfilling that agreement, we also must, in turn, have Business Associates Agreements with. We're a tiny company. I have a backup technical officer on monthly retainer in the event that I'm on vacation or sick or in the hospital and they need to have access to the data.  We have to have a background check and Business Associates agreement with that guy. Also, with our hosting provider, and any services we use. We build this legal chain of people swearing to comply with HIPAA. So, in that sense, it doesn't matter what the product is. It could be a Node app, a Rails app, or an Excel spreadsheet. It could be anything. We wrap this process of safe guarding around it. We do monthly audits, quarterly risk assessments, and yearly big picture reviews of our entire security posture. The owner of the company, the security officer, and myself form the security team.  We're a nine person company. I think if we were a thirty person company, three people could still do it. Three interested parties on the security team is the minimal responsible size. We do the log reviews every month. We talk about how good we're doing at patching and following our own security policies.  Then on the bigger stance, we talk about how good our security policies are. Are there holes, are they missing things? Are they unnecessarily restricting our employees from being able to do stuff? Then, once a year, or twice a year depending (I think we just bumped it up to twice a year), we're doing vulnerability scans and also third party risk assessments. We just had a third party security firm come in, look at everything and tell us we're actually doing really well, here's two spots we think you missed, and that we might want to get those fixed before the next time around. We identify the risk, talk through it, and we get things fixed right away. That's the security process around the HIPAA stuff.

We very specifically limit access for deployment and operations to that data. We have a list of employees within the company who have access to the database, the installation of that web app and a named list of people who have ssh access to the server, including the hosting company. Every month the hosting company sends me a report of who has logged into the server and why. They actually call me on the phone before they log into the server to get authorization. I'm really proud of our policy around access. We maintain separate laptops for anyone who has access to those servers. I have my normal laptop that I do my development work on, and that I'm talking to you on right now. Then I have a separate laptop running Ubuntu, with full disk encryption, with a YubiKey (I've actually got a YubiKey right here on my keychain), that unlocks the full disk encryption. I've got passwords on my SSH keys, and we have a VPN to get into the secure server. All of my accesses go into the audit log. We try and make sure we've done everything we reasonably can. Dealing with something like getting a virus, or getting socially engineered, or if someone actually got that laptop, we still have reasonable protections in place that maybe it wasn't actually a security breach. You have to assess each event as it happens. We try to build in these layers and layers and layers to absolutely minimize the worst case scenario which is that the full database gets stolen. My mindset when we have these quarterly meetings around our policies is: is it worth the hassle that I have to have a separate laptop?. I go from the scenario of: I've been subpoenaed, I'm in court and I'm having to explain to some of my peers why I did try hard enough.  If some of our data was stolen, here are all the steps I took. Was I obviously being sloppy, or was I careful enough? I look at it from that perspective.  Could I, with a clean conscience, explain to a group of my peers how did this happen and would they blame me for it, would they say I wasn't doing a good job, and that's why it happened.

**Jason**: That's interesting, even having two laptops, one that your primarily coding on and one for access, that's not a system. I've heard of situations where people only allow physical access to data centers.  You have to walk in, do your finger print scan, and all that stuff.  At some point, you make the process to hard to implement change in the system and the system breaks down.  Especially as you guys are small, it sounds like you've done a pretty good job building something that will help scale as you guys scale up to larger teams.

**Elliot**: I think so, and we're totally distributed. I haven't mentioned that yet. There's nine of us, but we're all in separate locations up and down the east coast of the US. I'm talking to you now from a tiny office co-working space in Portland Maine. The owners of the company are in Massachusetts. We have some other people in New Hampshire, Atlanta, and Philadelphia. Being totally dependent on physical location wouldn't work for us. I think that's a pretty bad model in general.

**Jason**: I agree.  I think a lot of times when you're starting something, you're operating the premise that I need to get something that I can get paid for as soon as possible. It's a fine line between how much do we push to just get something out and up, make sure we have a viable business, vs how much time do we spend setting things up the right way to help us in the future.

**Elliot**: It's a really interesting problem. I think our company has found what I think is a really nice balance in advancing the state of science. Pat is an adjunct professor at the Dartmouth Psychiatric Research Center. One of the problems that she noticed in the industry in general is everybody is 15 years behind science. We know how to help people better than we are currently doing, but what's being done in the field, is 15 years behind. One of the things that I'm really proud of us for doing is taking some of that know-how that science has figured out and actually wrapping it up and productizing it, and complying with HIPAA, putting it on servers, running it as software as a service, and making it easy for the smallest two bed clinic to get access to ground breaking tools and new ways of treating people.

**Jason**: Yeah, it's going to have a huge impact. To go back to one of our earlier questions, can you run through from a basic level, how you move through the environments.

**Elliot**: So I described the HIPAA production environment, but I never finished telling you about the other environments.  So we draw that really bright line around the production environment, and we put that over on the side and touch it as little as possible.  We have all these other environments.  We have some stuff running on Heroku, and we have some stuff running on standard VPS type servers at Digital Ocean, Linode, and Joyent. I'm planning to consolidate all of that onto Digital Ocean and AWS. Historical, some of it was from before I got here. Some of it was experimenting.  I think we're going to move a whole bunch of it onto AWS. We store everything in Git. We do development on our laptops. I have been working on introducing Chef into our environment. When I got here, everything was well documented in Wiki pages, just lists of commands to manually type to build a server. I knew I needed to get configuration management in place, but we have dozens of servers, not hundreds or thousands of servers. I didn't really have enough for a full blown, stand up a Chef server, kind of setup and invest a lot of time in it. I decided to introduce it to setup the developer laptops using Vagrant and Chef Solo. Once that was working, I refactored it into recipes that could be applied to the staging, production, and testing environments. We have automated tests running on push.  Depending on how many people we have working (our team grows and shrinks a little bit as we get funding for projects). If it's just me, I'll commit on master and make sure tests are green and then deploy to a staging environment.  We test everything out.  We'll deploy to a slightly larger environment and then into the production environment. When we have more people working, sometimes we'll bring up additional environments. We're just wrapping up this big push for a new wellness feature that's actually covering some physical health stuff as well as mental health. We brought up a whole extra environment where all the folks working on that new feature could iterate on it over several weeks. They have a place to push their work and to show the rest of the company, the non-engineers, as the feature was growing and getting solid. It's solid now, and we just merged it into master and now we're doing our last round of bug fixes before we push it to production. I try and make sure there's nothing blocking us to push an emergency bug fix to production. I try and minimize the number of branches that we have, but we do have branches for pr-production, current production, and testing.

**Jason**: Now what do you use for a testing framework.  are you on Rspec?

**Elliot**: We're using some Rspec and some Cucumber.  I've been experimenting a little with MiniTest. We actually have a totally stand alone Cucumber branch that we're using to test a lot of non Rails stuff. Six months ago, I started working with a QA engineer named Aaron. He brought in all this Cucumber stuff. We have stand alone Cucumber tests that check all our SSL expiration dates on every website, even the company blog. It checks that things are working ok. The Cucumber tests will go through and and check for things like a new blog post every week.  The Cucumber stuff has been really neat because it makes testing available to more of the business at this very high level. Inside the products themselves, it's a lot of Rspec.

**Jason**: Are you using any kind of a continuous integration server, or anything along those lines to build, or do you just run tests locally?

**Elliot**: We run locally, but we are using continuous integration.  I forgot the name of it. It's a really tiny one we're running internally. We're also using Tddium. I am planning on moving everything over to Travis.  I've been working with some friends here in the co-working space on some of their projects.  We got some other things setup in Travis and I just really like Travis. It's almost fun to use. They did a really good job. We're also using Code Climate. Code Climate does a security scan on your code. I haven't been totally happy with continuous integration that I've been doing the last two years because at Canonical, we did such a good job with continuous integration. We had this system called PQM, which stood for Patch Queue Manager, and it would make sure tests were green before merge into trunk. There is this big philosophical debate in continuous integration around: do my tests need to be green before I merge to trunk, or do I have to feel pressure to fix them immediately if they break once they're in the trunk? I'm a big believer in the system that your branch should be rejected from merging if the tests aren't green. Don't let it into trunk, because  you're just putting the problem right in the middle where everyone has to deal with it. I think the Open Stack project has done some really good work with pre-merge testing.  That's one of the things I really like with Code Climate. Code Climate just recently started hooking on GitHub pull requests and then commenting on your pull request with the security analysis of that pull. So it lets you know things like: this introduces two new mass assignment issues. I really like that. I haven't figured out how to do that with Travis yet, I think it might support the same sort of thing.  What I would really like is to look at a set of merge requests and know that it's green for tests and green for security. If I like the code change, I'm going to merge it.

**Jason**: My current company, right now, Gazelle, use CircleCI. We've started to switch everything over from an internal Jenkins server to CircleCI, which is similar to Travis. One of the things they let you do is add hooks. For all of our gems, once the specs go green, we'll push the gem out to our gem server as part of the final test.  So the goal being that no developers build gems, gems are only built if tests pass. We use Code Climate, I think it's awesome, the gamification, and then alerting you to stuff, but the same thing, down the road I'd like to see if we can get it to where once Circle has run, if everything is green, then push, either tag it or push, create some sort of action to act on.

**Elliot**: Some people argue, well why are you going to pay $50 a month/$100 a month to Code Climate. You can just run BrakeMan yourself. I could run BrakeMan myself, but if I pay, there is a team of people who are totally focused on doing the best security scanning that they can. They also keep developing, adding things like integration with pull requests, and scanning your javascript, and whatever feature they release next month. I feel like it's totally worth it to get that level of niche expertise.  I would never invest that level, I might run BrakeMan, but I would never realistically, in a small company like this, be able to invest that level of effort in improving the quality of the scanning.  Using a service, I know I'm going to get improvements coming in.
**Jason**: Exactly.  We ran into that problem with our gem server. I setup a server with a project called Gem In a Box, which is just simple gem management software.  It allow you to push from the command line, something that was really helpful doing development.  We ended up moving it over to Gem Fury, because it was one of those things. It was another machine we had to setup and secure and lock down.  It was a question of, can we do it cheaper on AWS, of course, but then you have more to manage.  It is an interesting conundrum.  You want to keep as much in house that you need to keep in house, but you want to push the stuff that's really not focused on your core product, if you can afford it.

**Elliot**: Yeah, you should constantly reevaluating whether you're spending your brain cycles in areas that are adding the most value to your business. It's really good to have the technical depth of being able to move things back in-house. You also need to be wary of setting yourself up in a vendor lock-in situation, or painting your self into a corner. It's important to not be too egotistical about it, like, "I built this entire thing by myself". It's great, but no one else can maintain it, and you didn't spend any time building features the business cares about.

**Jason**: Can you give me a quick overview of what you guys use for deployment, and what your deployment looks like. For example, do you use Capistrano or a tool like that?

**Elliot**: We're using Capistrano and I mentioned before, we're using Vagrant with Chef Solo. I'm using Chef Solo with encrypted data bags to manage secrets, like SSL private keys. The product that we have on Heroku, deploys with git push. Where I want to go, is PaaS. PaaS, PaaS, PaaS, everywhere, on my personal project, on the HIPAA stuff, on our more generic applications, I want everything to be a PaaS. If I was running a larger team, I would have an ops team. The ops team runs the Paas. The development team deploys to the Paas. Dev team controls the deployments, the ops team runs the lower level, service stuff.  So, I'm not sure how to get there yet. I've been watching Docker really carefully.  One of the drawbacks to Vagrant is that it takes a while for the configuration management to run and to boot a new machine.  I've been looking, with great interest at Packer, Mitchell's new tool.  I think Packer can also be combined with Chef to pre-build those environments, and I think Packer can also build Docker images now.

**Jason**: Yeah, it can now.

**Elliot**: I'm not sure how all of that is going to reconcile with the Heroku build pack model. No matter how it happens, I'm totally committed to getting all of my project running on a PaaS.  They may have to be different PaaS, right, it might have to be a small, private PaaS that I run myself that is HIPAA compliant, but that's the model I want.  I want be that when I'm deploying my product, I do a git push. When I'm bringing up a new experiment or prototype, I pick which build pack it is and I do a git push.  I think there is work left to do to make the local developer experience that easy, and to make the HIPAA product experience that easy. If I've got my non-HIPAA compliant product, I can spin up a new app on Heroku in like 30 seconds, and it's super delightful.  It's actually much more painful as a local developer on the laptop to get something up and running. I would like to have that same local ease of use. I would like to see that ease of use spread in both directions: to the local developer laptop, and to the ultra-secure environment.

**Jason**: Your system now, do you have it setup so you can do isolated development, or do you need to have all the parts up and running to do development?

**Elliot**: I have it setup so I can do isolated development on individual products.  I'm a big fan of factoring out services.  I think this is one of those decisions where you have to constantly re-evaluate.  Some of the folks at Etsy were talking about how they've actually swung the opposite direction from being service oriented and having lots of these micro services, to moving back to having more monolithic apps.  I think you need to look at it depending on what you're trying to optimize for. Are you trying to optimize for performance, for developer productivity, one developer being able to maintain fifteen different apps?  In general, I'm a huge fan of the Twelve-Factor app philosophy that underlies Heroku.  If you read through that stuff, regardless if your using their tools or not, it just makes so much sense to factor your tools out that way.  I try to make it so I can and do run every single service on my laptop at the same time, but that I can run them individually, and work on them individually if needed. I really like to spin up the database and check some stuff happening with the database without needing to be running all the other pieces. If I am working on it all at once, or if I'm debugging some interaction, I want to be able to get all the pieces up and running. Currently, I have a Vagrant machine for our product called Recovery Library, and one for our product called Common Ground. They use different ports, so I can run them at the same time.  Within there, I can debug all the pieces, which have been stitched together using Chef Solo.

**Jason**: Have you used Chef to setup any of the other environments?

**Elliot**: Yes, some of our other server environments are also using Chef Solo.  I have this model where I use community cookbooks for some of the really common things like the database, Apache server or the Nginx server. For each product, I have a custom cookbook.  It took me a while to get to this point. My custom cookbook is not reusable. It's full of hard coded application and, company specific stuff. Once it's working, I go back and look at it to see what I might be able to refactor out. I always start by looking at the really general things, like the ElasticSearch cookbook, and then the really company specific stuff: I might put all of my own overrides in and tweak things that are really specific to our environment. Every environment that I build has one product specific cookbook that has all the overrides and stuff.

**Jason**: Through my process of learning, I've come to the exact same point. I feel like I keep refactoring. We have a core cookbook that sets up a generic Rails servers. I think that's been one of the challenges with Chef. It's so configurable and so powerful. I talked to one of the engineers from Opscode, who is part of the community group. He told me that Opscode doesn't use a lot of their own public cookbooks internally. I'm not blaming them at all, they've provided an incredible resource, but I'd run across these bugs that it felt like if you'd run this thing, you would have come across the bug. It sounds like they are going to start pulling a lot more of the public cookbooks into their internal process.

**Elliot**: I think you need configuration management, you need a tool like that to configure your PaaS platform. If I really step back and think about it, do I really want to teach the next ten engineers that come into the company how to use Chef (or Puppet, or JuJu, or BOSH). Looking at the experience you get from Heroku or Cloud Foundry, it's not worth it at all. They shouldn't know a line of Chef.  They should go, "I want one of these types of services", turn it on, and here are the credentials.  I want it to go this fast or that fast.

**Jason**: Heroku has spoiled everybody to what can be.

**Elliot**: I think they've stumbled onto something.  The PaaS idea has been floating around for years. I think it really is the right level of abstraction.  It just fits so naturally.  Now I love the operations side.  I'm always going to want to know how to build the PaaS, what's underneath it, and how to do the configuration management stuff.  When I look at what I'm going to require every engineer on the team to learn, I'm not sure that making everyone learn configuration management is really a good idea. I do want everyone to do deploys and to spin up new prototype products, but I'm not sure everyone building a new server is the right thing.

**Jason**.  In this day and age, as developers, the breadth of what we're required to deal with, or what's part of the job description has gotten a lot wider. Historically there have been Database Administrators who have focused on the databases, and IT specific operations folk who build and manage machines. The developers just wrote the code and then pushed it. This convergence, I think is both a good thing and a bad thing. It's a good thing in that as engineers, we know a lot more about all the things we touch, but the downside is there is no way we can become experts in this incredibly wide swath of knowledge.

**Elliot**: And there is this incredible pressure on the front-end as well. I'm writing an HTML5 web application. It's portable, it's responsive, and it's beautiful, and now you're trying to debug hardware accelerations on a particular Android device, or trying to make use of sensors through some Javascript plugins. There are Ubuntu phones coming out soon, and Windows phones. You're trying to server real, genuine clients, who are out in the field treating people in healthcare (in my case), who have chosen a particular type of tablet. I have to really care about making sure our site actually works on that device, regardless of how standards compliant those browsers on those devices are.

**Jason**: It's an interesting challenge. We're getting near the end of the hour, and I want to
make sure I get some of my last questions in. You talked a little bit about keeping credential in encrypted data bags. Could you talk a little bit more about how you store credentials and configuration across environments?

**Elliot**: I have one encrypted data bag for each product.  Within there, there are sub-sections for environments, so here are my staging credentials, here are my production credentials. Within these environments, I list the things like the SSL key, S3 credentials and whatever else.  The recipe goes and reads the encrypted stuff out of the data bags while Chef is running and lays it down in an environment file. I'm using the Figaro gem to pull it into the application.  The reason I like it is because it works the same on Heroku as it does on my old school servers. From the application code's perspective, I'm looking in the environment dictionary and grabbing some keys. I have sane defaults that we just put into Git.  For developers, there are throw away keys, passwords are ABC, etc. Everything works with no special encrypted data bag. When we go to build out a new production machine, it drops the right keys in. If I need to rotate keys, I just change the keys in the data bag, go re-run Chef.  It bounces the servers, and everything starts with the new keys.

**Jason**: Part of why I want to do these talks is it feels like there are a lot of these key pieces that are really critical, but there's not a lot of discussion around.  We're all trying to figure this stuff out.

**Elliot**: There's not. You can have this really opinionated discussion around configuration
management. Somebody's pushing Ansible and somebody's pushing Chef, and somebody's talking about Salt, and somebody's talking about Puppet.  For me it's really simple: tell me a story about how you deploy an SSL certificate.  That's all I want to know.  How do you deploy an SSL certificate, how do you deploy an S3 credential? It totally changes the conversation. I think we tend to talk about getting an app server up and running.  It's cool, but it's not the full story. That thing doesn't work.  You can't use it without securing it and having real production credentials.

**Jason**: A question I think I glossed over, how do you harden servers?

**Elliot**: I feel like we could really benefit from a shared, industry wide, set of checklists that's peer reviewed and people can contribute to over time.  This is the sort of thing that's challenging because there is just too much to remember if you just think about it.  If we're all maintaining our own private checklists, then we're not benefiting from other people's clever ideas. I benefit from having third party security reviews. The tips I get in those security reviews are not proprietary and confidential.  I feel like personally, I should be doing more to publish some of these.  I have internal checklists of a set of stuff we do to servers: settings we set on the web server, changes we make to the Ubuntu configuration, settings we have in the database, and scans that we run using Nessus.  We have this checklist and we do these three month reviews and run through all these scenarios, and I'll think of something to add to the checklist, or someone will do a review and find something to add to the checklist. It's not really published anywhere. I think someone put a GitHub project up that was a whole bunch of Nginx configurations.  We really should be doing more of that.

**Jason**: There is a good Postgres one too, it's more on the tuning side, but it's bunch of changes based on memory and cpu cores; here are some general settings you should be playing with. The default are too conservative.

**Elliot**: Then again, I feel like that should be abstracted behind the PaaS.

**Jason**: Very true.  My hope for these conversations is exactly this.  Let's start to document what different people are doing, and as a community, let's start trying to figure some of these problems out.  At least if we have a conversation going on where everyone's contributing, there will be some nuggets of information that will really help.

**Elliot**: One of the things that we could really do to help the community of developers who are trying to change the world with the products they are working on is make it easier for new people to get started with the hardest problems, which are around operations and security and hardening. I think if we don't talk about those and don't share that stuff, and push it off as a really hard problem that I hope I don't get stuck fixing, then it's a much higher barrier for people who are less expert at development to get involved in doing those things. Where I see this huge gap is in the people who aren't engineers, who have this incredible, lived experience in these various areas (health care, energy, etc.) try to contribute. It should be a lot easier for one of them to pair up with an engineer and bring something to market. To have a two person startup be able to bring a medical app to market would be fantastic, but it means you need a HIPAA compliant PaaS you can point to, and it solves that stuff for you.

**Jason**: I've been thinking for a long time about putting together a series of cookbooks.  There are definitely some missing ones, especially the hardening aspects, to setup a basic Rails environment. A real environment with a database, with and SSL certificate, with environment variables. When Rails came out, I was writing a lot of PHP. I was one of those PHP programmers who spit out a lot of spaghetti PHP code. Once I started working with Ruby and Rails, I found my PHP code got a lot cleaner and structured because I had a model to go from.  Someone who was much smarter than me said: "Hey, try this, this works well in the real world". The design was nothing new, the MVC design paradigm has been around for a long time, but Rails provided a concrete example. I think what dev-ops needs is a concrete example. We know we can piece all the parts together, but walking in new and fresh, you don't necessarily know what you should piece together.  What the downstream ramification of choosing this part over that part will be. We keep making everything more and more modular, and then we keep building higher and higher level abstractions on these. This allow us to focus. Like you said, a two person team can start something very big or solve a problem that really big without spending all this money and time setting up all this infrastructure.

**Elliot**: I think we should be willing to pay $500 for a set of template configs or $1,000 for a really good application template.  When you think about the number of hours that would save a new product team, and the number of embarrassing mistakes it would save them. I'm not sure what structure we could setup that would pay someone to pull all those pieces together and curate them and then sell them back out to the community, because they are all open source.  I think it's worth paying somebody to actually work on that.  That's kind of why I like paying for some of those services like Paper Trail and Code Climate. It means there is a team actually working really hard on constantly tuning and refining that set of stuff.

**Jason**: That's a really good point.  I listen to the FoodFight show, and I peruse GitHub. I'm constantly looking for key vendors who are pushing out good quality cookbooks.  I have try to bookmark, but that's not a great system. My plan for this talk is to transcribe it, and like they do on the Food Fight show, put together a list of topics that we talked about.  Try to start this as more of an organic resource, a blog, and a place to start documenting a common set of tools.
My final question: Is there anything you'd like me to ask, to my list of questions for when I'm talking to people?

**Elliot**: There's two things I'm trying to learn about right now. One is going to totally solve itself; figuring out Packer and Chef and the containers, like Docker images.  A lot of people are thinking about that stuff. Where I have a really hard time getting expert experience is running file system level encryption in the cloud and doing client side encryption. With all the events unfolding in the past year, I'm worried more than ever, are we doing enough to protect our clients, our customers, who are this vulnerable population. If you can encrypt the data before it even gets into the database, then the database doesn't have to be HIPAA compliant, because you're handling that encryption on a different layer. I'm planning to go to Real World Crypto in January in NY City. There's a bunch of interesting talks from browser vendors talking about security in crypto. I think, if anyone out there is actually doing file system level encryption on their servers in the cloud, how do you handle key rotation, how do you handle auditing of who's accessed those keys.  It's a real grey area, or dark art to setting that stuff up.

**Jason**: That's really interesting point.  Obviously HIPAA compliant stuff is an area where that's currently present.  We seen a move over to running everything over SSL rather.

**Elliot**: But not even running SSL. All your traffic just got diverted through Iceland and now someone else can steal your key and decode the replay. Are you running forward secrecy. Sure you're running encrypted sessions and you have hashed passwords, but are you running encryption by default everywhere you reasonable could be.  I think were we are as an industry now is that there are a lot of places we reasonable could be running encryption because we have the tools, but the know how just isn't out there.  Most engineers, myself included, have no idea how to set this stuff up.

**Jason**: I think the number is much higher than nine out of ten.  Generally you don't need to, and you figure it out as you go.  Elliot, thank you very much for your time.  I've enjoyed our talk today.
